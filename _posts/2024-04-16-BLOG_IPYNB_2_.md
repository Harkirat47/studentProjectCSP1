---
toc: True
comments: True
layout: post
title: Data Structures Writeup
description: Blog for CSP week 29
courses: {'csp': {'week': 29}}
type: hacks
---

# SQLite Database Table Creation:
Using VSCode and SQLite3 Editor, create a table in your SQLite database to store your collection data.
Define the columns in your table to represent the attributes of the collection items.
You might create a table named collections with columns like id, name, description, etc.

# Collections

# _________________________________________________________________________________________________________

## This is the Database of Images genorated by the model 
<img src="../../../images/IDB.png" alt="image13" style="border: 2px solid black; width: 1000px;">

From VSCode model, show your unique code that was created to initialize table and create test data.
See Code Below
Code initializes three users, two default ones as requested by teacher, and an admin account for personal use.


```python
def initEasyImages():
    with app.app_context():
        db.create_all()  # Create all tables if they don't exist
        # Provide paths and metadata for images
        images_data = [
            {"path": "link.jpg", "_xCoord": 250, "_yCoord": 250, "_difficulty": 0},
            {"path": "link.png", "_xCoord": 250, "_yCoord": 250, "_difficulty": 0}
        ]
        # Create image instances based on the provided data
        images = [Images(**data) for data in images_data]
        # Add images to the database
        for image in images:
            try:
                image.create()
                print("Successfully added entry")
            except:
                db.session.remove()
                print("Error adding image: ", image.imagePath)

```

## This is the code for the model with a function called initEasyImage that adds 2 images as Meta data as seen in the picture above.


```python
# Import necessary modules
from sqlalchemy import Column, Integer, String, Text, LargeBinary
from sqlalchemy.exc import IntegrityError
from pathlib import Path
from __init__ import app, db  # Import Flask app and SQLAlchemy database instance

# Define the Images table in the database
class Images(db.Model):
    __tablename__ = "images"
    id = db.Column(db.Integer, primary_key=True)
    _xCoord = Column(Integer, nullable=False, default=250)  # Default x-coordinate
    _yCoord = Column(Integer, nullable=False, default=250)  # Default y-coordinate
    _difficulty = Column(Integer, nullable=False, default=0)  # Default difficulty level
    imageData = db.Column(db.Text, nullable=True)
    imagePath = db.Column(db.Text, nullable=True)

    # Constructor to initialize an image object
    def __init__(self, imagePath, imageData=None): 
        self.imagePath = imagePath
        self.imageData = imageData

    # Method to represent the image object as a string
    def __repr__(self):
        return f"<image(id='{self.id}', imagePath='{self.imagePath}')>"

    # Method to convert image object to a dictionary
    def to_dict(self):
        return {"id": self.id, "imagePath": self.imagePath}

    # Method to add an image to the database
    def create(self):
        try:
            db.session.add(self)
            db.session.commit()
            return self
        except IntegrityError:
            db.session.remove()
            return None

    # Method to read the details of an image
    def read(self):
        return {
            "path": self.imagePath
        }

    # Method to update the details of an image
    def update(self, path=""):
        if path:
            self.imagePath = path
        db.session.commit()
        return self

    # Method to delete an image from the database
    def delete(self):
        db.session.delete(self)
        db.session.commit()
        return None

# Function to initialize images in the database
def initEasyImages():
    with app.app_context():
        db.create_all()  # Create all tables if they don't exist
        # Provide paths and metadata for images
        images_data = [
            {"path": "https://t3.ftcdn.net/jpg/03/95/29/32/360_F_395293226_A4boRgABAbfXmAmmynQHcjjIIB3MjDCj.jpg", 
             "_xCoord": 250, "_yCoord": 250, "_difficulty": 0},
            {"path": "https://purepng.com/public/uploads/large/purepng.com-super-mariomariosuper-mariovideo-
             gamefictional-characternintendoshigeru-miyamotomario-franchise-17015286383789a9am.png", "_xCoord": 250, "_yCoord": 250, "_difficulty": 0}
        ]
        # Create image instances based on the provided data
        images = [Images(**data) for data in images_data]
        # Add images to the database
        for image in images:
            try:
                image.create()
                print("Successfully added entry")
            except:
                db.session.remove()
                print("Error adding image: ", image.imagePath)

# Call the function to initialize images
initEasyImages()

```

# Lists and Dictionaries
Blog Python API code and use of List and Dictionaries.

In VSCode using Debugger, show a list as extracted from database as Python objects.
GET request is sent to backend to search for all public designs. Backend fetches all public designs into a list in python debugger called design_return (red line). List contains all designs as python objects (red line).

# list as python objects

In Python, a list is a versatile and mutable collection of items. Here's what you need to know about lists as Python objects:

Mutable: Lists are mutable, meaning they can be modified after they are created. You can add, remove, or change elements in a list.
Ordered: Lists maintain the order of elements. The order in which elements are added to the list is preserved, and you can access elements by their index.
Heterogeneous Elements: Lists can contain elements of different data types, such as integers, floats, strings, or even other lists.
Dynamic Sizing: Lists in Python are dynamic in size, meaning they can grow or shrink as needed. You can add or remove elements without specifying the size beforehand.

<img src="../../../images/123.png" alt="image13" style="border: 2px solid black; width: 1000px;">

# Dictionary Keys

In Python, a dictionary is a data structure that stores a collection of key-value pairs. Each key in a dictionary must be unique, and it is used to access its corresponding value. Here's a breakdown:

Key: A key is an immutable (unchangeable) data type such as a string, integer, float, or tuple. It serves as an identifier or label for the corresponding value in the dictionary. Keys must be unique within a dictionary, meaning that no two keys can be the same.
Value: A value is associated with a key in a dictionary. It can be of any data type, including strings, numbers, lists, tuples, dictionaries, or even functions.
Key-Value Pair: A key-value pair consists of a key and its corresponding value in the dictionary. It's essentially a mapping between the key and its associated value.

<img src="../../../images/1232.png" alt="image13" style="border: 2px solid black; width: 500px;">

# API + Json

Blog Python API code and use of Postman to request and respond with JSON.

In VSCode, show Python API code definition for request and response using GET, POST, UPDATE methods. Discuss algorithmic condition used to direct request to appropriate Python method based on request method.
Within the code shown above, the API contains several CRUDs, such as a CRUD for modifying users and one for modifying Designs.
A resource is then added to the API under the appropriate link.
When a request is sent to the link, the appropriate function is called according to the type of request send.


```python
# Add resources outside the class definition
images_api.add_resource(ImagesAPI, '/')
images_api.add_resource(PostImagesAPI, '/upload')
```

In VSCode, show algorithmic conditions used to validate data on a POST condition.
Algorithmic conditions ensure that inputted data is valid. The following two conditions are part of the user creation code. They ensure that the password is secure by ensuring that it is longer than a certain length, and ensure that a Name and password exists.


```python
# Validate name
name = body.get('name')
if name is None or len(name) < 2:
    return {'message': 'Name is missing or is less than 2 characters'}, 400

# Validate uid
uid = body.get('uid')
if uid is None or len(uid) < 2:
    return {'message': 'User ID is missing or is less than 2 characters'}, 400
```

In Postman, show URL request and Body requirements for GET, POST, and UPDATE methods.
In Postman, show the JSON response data for 200 success conditions on GET, POST, and UPDATE methods.

## Post
<img src="../../../images/post.png" alt="image13" style="border: 2px solid black; width: 1000px;">

## PUT
<img src="../../../images/put.png" alt="image13" style="border: 2px solid black; width: 1000px;">

## GET
<img src="../../../images/get.png" alt="image13" style="border: 2px solid black; width: 1000px;">

## 404
<img src="../../../images/posterror.png" alt="image13" style="border: 2px solid black; width: 1000px;">


# frontend

## GET

<img src="../../../images/12.png" alt="image12" style="border: 2px solid black; width: 1000px;">


```python
// Update the apiUrl to the correct endpoint in your backend server
const apiUrl = 'http://127.0.0.1:8086/api/images/'; // Update this URL

function downloadImage(imageUrl) {
  // Create a temporary anchor element
  var a = document.createElement('a');
  a.href = imageUrl;
  a.download = 'image.jpg';
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
}

// Get token from cookies
const token = getCookies()['token'];

if (token) {
  fetch(apiUrl, {
    headers: {
      'Authorization': `Bearer ${token}`
    }
  })
  .then(response => {
    if (response.ok) {
      return response.json();
    } else {
      throw new Error('Token validation failed');
    }
  })
  .then(data => {
    const galleryContainer = $('#gallery_container');
    const imagesPerPage = 4; // Change this value to the desired number of images per page

    // Calculate the total number of pages
    const totalPages = Math.ceil(data.length / imagesPerPage);

    data.forEach((item, index) => {
      if (index % imagesPerPage === 0) {
        // Create a new page when needed
        const pageNum = index / imagesPerPage + 1;
        const pageLink = $(`<a href="#" data-page="${pageNum}">${pageNum}</a>`);
        pageLink.appendTo('#pagination_container');
      }

      var card = $('<div class="card"></div>'); // Create a card container
      var image = $('<img class="img">'); // Create an image element

      image.attr("src", "data:image/jpeg;base64," + item.imageData); // Set image source
      image.appendTo(card); // Append image to card

      // Adding click event listener to each image for enlarging
      image.on('click', function() {
        $('#lightbox_img').attr('src', this.src);
        $('#lightbox').fadeIn();
        $('.overlay').fadeIn();
        $('.download-button').fadeIn(); // Show download button
      });

      // Adding click event listener to download button
      $('.download-button').on('click', function() {
        downloadImage($('#lightbox_img').attr('src')); // Call downloadImage function with image source
      });

      card.appendTo(galleryContainer); // Append card to gallery container
    });

    // Add event listener for pagination
    $('#pagination_container a').on('click', function(e) {
      e.preventDefault();
      const pageNum = parseInt($(this).attr('data-page'));
      showPage(pageNum);
    });

    // Function to display the images for the selected page
    function showPage(pageNum) {
      galleryContainer.children('.card').hide(); // Hide all images
      galleryContainer.children(`.card:nth-child(n+${(pageNum - 1) * imagesPerPage + 1}):nth-child(-n+${pageNum * imagesPerPage})`).show(); // Show images for the selected page
    }

    // Initially show the first page
    showPage(1);
  })
  .catch(error => console.error('Error fetching images:', error));
} else {
  // Handle case when token is not available
  console.log('Token not available. Please login.');
}

// Function to get cookies
function getCookies() {
  var cookies = {};
  document.cookie.split(';').forEach(function(cookie) {
    var parts = cookie.split('=');
    cookies[parts.shift().trim()] = decodeURI(parts.join('='));
  });
  return cookies;
}

// Close lightbox when close button is clicked
$('#closeLightbox').on('click', function() {
  $('#lightbox').fadeOut();
  $('.overlay').fadeOut();
  $('.download-button').fadeOut(); // Hide download button
});
```


 This JavaScript code retrieves a JWT token from cookies and uses it to authenticate a GET request to a backend server's endpoint (apiUrl) to fetch image data. Upon successful authentication, it processes the received JSON data to dynamically generate a gallery of images, displaying a specified number of images per page with pagination. Each image can be clicked to enlarge it in a lightbox, and there's an option to download the enlarged image. The pagination functionality allows users to navigate through multiple pages of images. Additionally, it includes error handling to manage cases where token validation fails or the fetch request encounters an error, and it provides a function to extract cookies and another to close the lightbox when a close button is clicked.

## Post 

<img src="../../../images/13.png" alt="image12" style="border: 2px solid black; width: 1000px;">


```python
function handleFiles(files) {
    // const myHeaders = new Headers();
    // myHeaders.append("Content-Type", "application/json");

    for (let i = 0; i < files.length; i++) {
      const file = files[i];
      if (file.type.match('image.*')) {
        const reader = new FileReader();
        reader.onload = function (event) {
          const fileName = file.name;
          const base64String = event.target.result.split(',')[1];
          console.log(base64String); // Log base64 representation
          
          // Send base64String to server
          fetch('http://localhost:8086/api/images/upload', {
            method: 'POST',
            headers: {
              "Content-Type": "application/json"
            },
            body: JSON.stringify({ 
              base64_string : base64String,
              name: fileName, 
            })
          })
          .then(response => {
            if (response.ok) {
              return response.json();
            } else {
              throw new Error('Upload failed');
            }
          })
          .then(data => {
            // Handle successful upload
            console.log(data);
            alert('Image uploaded successfully');
          })
          .catch(error => {
            // Handle errors
            console.error(error);
            alert('Upload failed. Please try again.');
          });
        };
        reader.readAsDataURL(file);
      } else {
        alert('Please select an image file.');
      }
    }
  }
```

 This JavaScript function handleFiles is designed to handle files selected by the user, particularly image files. When files are passed to this function, it iterates over each file using a for loop. For each file, it checks if the file type matches that of an image using the file.type.match('image.*') condition. If it's an image file, it proceeds to read the contents of the file using a FileReader. Upon successful reading of the file, it extracts the file name and converts the image data into a base64-encoded string. This base64 string, along with the file name, is then sent to the server via a POST request to the specified endpoint (http://localhost:8086/api/images/upload). The request includes the base64 string and file name in JSON format in the request body. After the upload request is made, it handles the response: if the response is successful, it logs the response data and displays an alert indicating successful upload; if there's an error in the upload process, it logs the error and displays an alert indicating upload failure. This function provides a convenient way to upload images to the server asynchronously.

# Data Preparation and Analysis in Machine Learning

## Data Preparation for Analysis:

- **Data Cleaning:**
    - Imputation: Filling missing values with a statistical measure like mean, median, or mode.
    - Dropping: Removing rows or columns with missing values or duplicates.
    - Outlier Detection: Identifying and handling outliers, either by removing them or transforming them.

- **Encoding Categorical Variables:**
    - Label Encoding: Assigning a unique integer to each category. May introduce ordinality.
    - One-Hot Encoding: Converting categorical variables into binary vectors.

## Algorithms for Analysis:

- **Linear Regression:**
    - Ordinary Least Squares: Minimizing the sum of squared differences between observed and predicted values.
    - Coefficient Interpretation: Understanding the impact of feature coefficients on the target variable.

- **Decision Trees:**
    - Tree Structure: Nodes represent features, branches represent decisions, and leaves represent outcomes.
    - Splitting Criteria: Determining the best feature and value to split the data at each node.
    - Pruning: Techniques to prevent overfitting by simplifying the tree.

## Data Preparation for Predictions:

- **Feature Scaling:** Scaling numeric features to a similar range.
- **Model Training:** Splitting data into training and testing sets for model evaluation.
- **Prediction:** Making predictions on new, unseen data.


# Example 
#  _________________________________________________________________________________________________________________________________________

In the ML projects, there is a great deal of algorithm analysis. Think about preparing data and predictions.

Show algorithms and preparation of data for analysis. This includes cleaning, encoding, and one-hot encoding.
Below code demonstrates data cleaning in titanic ML project
Garbage In, Garbage Out, if bad data is fed in bad data will come out therefore we need to clean data and remove bad datapoint
Encoding: data may come in different forms, i.e. 1, male, female, we need to turn these all into numbers so that model can function, model functions only with numbers, does not work well with other data types.


```python
def _clean(self):
        # Drop unnecessary columns
        self.titanic_data.drop(['alive', 'who', 'adult_male', 'class', 'embark_town', 'deck'], axis=1, inplace=True)

        # Convert boolean columns to integers
        self.titanic_data['sex'] = self.titanic_data['sex'].apply(lambda x: 1 if x == 'male' else 0)
        self.titanic_data['alone'] = self.titanic_data['alone'].apply(lambda x: 1 if x == True else 0)

        # Drop rows with missing 'embarked' values before one-hot encoding
        self.titanic_data.dropna(subset=['embarked'], inplace=True)
        
        # One-hot encode 'embarked' column
        onehot = self.encoder.fit_transform(self.titanic_data[['embarked']]).toarray()
        cols = ['embarked_' + str(val) for val in self.encoder.categories_[0]]
        onehot_df = pd.DataFrame(onehot, columns=cols)
        self.titanic_data = pd.concat([self.titanic_data, onehot_df], axis=1)
        self.titanic_data.drop(['embarked'], axis=1, inplace=True)

        # Add the one-hot encoded 'embarked' features to the features list
        self.features.extend(cols)
        
        # Drop rows with missing values
        self.titanic_data.dropna(inplace=True)

```

Show algorithms and preparation for predictions.
Functions below use decision tree classifier and linear regression to train model
First function trains model, second one transforms inputted dataset to a data frame array, and then runs a prediction using the previously trained model


```python
def _train(self):
    # split the data into features and target
    X = self.titanic_data[self.features]
    y = self.titanic_data[self.target]
    
    # perform train-test split
    self.model = LogisticRegression(max_iter=1000)
    
    # train the logistic regression model
    self.model.fit(X, y)
    
    # train a decision tree classifier
    self.dt = DecisionTreeClassifier()
    self.dt.fit(X, y)

def predict(self, passenger):
    # clean the passenger data
    
    # Create a DataFrame with the passenger data
    passenger_df = pd.DataFrame(passenger, index=[0])
    
    # Convert 'sex' column to binary (1 for male, 0 for female)
    passenger_df['sex'] = passenger_df['sex'].apply(lambda x: 1 if x == 'male' else 0)
    
    # Convert 'alone' column to binary (1 if passenger is alone, 0 otherwise)
    passenger_df['alone'] = passenger_df['alone'].apply(lambda x: 1 if x == True else 0)
    
    # Perform one-hot encoding for 'embarked' column
    onehot = self.encoder.transform(passenger_df[['embarked']]).toarray()
    cols = ['embarked_' + str(val) for val in self.encoder.categories_[0]]
    onehot_df = pd.DataFrame(onehot, columns=cols)
    passenger_df = pd.concat([passenger_df, onehot_df], axis=1)
    
    # Drop unnecessary columns ('embarked', 'name')
    passenger_df.drop(['embarked', 'name'], axis=1, inplace=True)
    
    # Predict the survival probability using logistic regression model
    die, survive = np.squeeze(self.model.predict_proba(passenger_df))
    
    # Return the survival probabilities as a dictionary
    return {'die': die, 'survive': survive}

```

### Key Concepts in Regression:

1. **Dependent Variable (Outcome)**:
   - The dependent variable (often denoted as \(Y\)) is the variable we want to predict or explain. It's the outcome of interest in our analysis. For example, in housing prices prediction, the price of the house might be the dependent variable.

2. **Independent Variables (Predictors)**:
   - Independent variables (often denoted as \(X\)) are the variables that we believe may have an influence on the dependent variable. These are the factors we use to predict or explain variations in the outcome. In the housing prices example, independent variables might include features like square footage, number of bedrooms, location, etc.

3. **Regression Equation**:
   - The regression equation represents the relationship between the independent variables and the dependent variable. It's expressed mathematically as:
     \[ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon \]
     - \(Y\) is the dependent variable.
     - \(X_1, X_2, ..., X_n\) are the independent variables.
     - \(\beta_0, \beta_1, \beta_2, ..., \beta_n\) are the coefficients (parameters) representing the strength and direction of the relationship between the variables.
     - \(\epsilon\) is the error term, representing the difference between the observed and predicted values.

4. **Assumptions**:
   - Regression models are based on several assumptions, including linearity, independence of errors, homoscedasticity (constant variance of errors), and normally distributed errors. Violations of these assumptions can affect the validity and reliability of the regression results.

5. **Estimation of Parameters**:
   - The goal of regression analysis is to estimate the parameters (\(\beta\) coefficients) of the regression equation that best fit the data. This is typically done using techniques like Ordinary Least Squares (OLS) for linear regression, which minimizes the sum of squared differences between observed and predicted values.

6. **Interpretation of Coefficients**:
   - The coefficients (\(\beta\) parameters) in the regression equation represent the change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant. Positive coefficients indicate a positive relationship, while negative coefficients indicate a negative relationship.

7. **Prediction**:
   - Once the regression model is fitted, it can be used to make predictions on new or unseen data. By inputting values of the independent variables into the regression equation, we can obtain predicted values for the dependent variable.

8. **Model Evaluation**:
   - Regression models need to be evaluated to assess their predictive accuracy and goodness of fit. This involves examining measures such as R-squared (for explaining the proportion of variance in the dependent variable), residual analysis (to check for the randomness of errors), and hypothesis testing on coefficients.

In essence, regression analysis provides a framework for quantifying relationships between variables, making predictions, and understanding the underlying patterns in data. It's a versatile tool used in various fields such as economics, finance, social sciences, and more, to analyze and interpret complex relationships in data.



<img src="https://images.spiceworks.com/wp-content/uploads/2022/04/07040339/25-4.png" alt="image12" style="border: 2px solid black; width: 1000px;">

Discuss concepts and understanding of Decision Tree analysis algorithms.
Decision tree involves terminal nodes, or inputs
Decision tree has decision nodes that make decisions based upon terminal nodes and inputs. These nodes make another output based upon inputs and send it out. All nodes eventually converge into a root node that has final decision of tree

<img src="https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/DT.png" alt="image12" style="border: 2px solid black; width: 1000px;">

### Key Concepts in Decision Trees:

1. **Decision Tree Structure**:
   - A decision tree is a hierarchical structure consisting of nodes that represent decision points and branches that represent possible outcomes or paths. At each decision node, a decision is made based on the value of a particular feature.

2. **Root Node**:
   - The top node of the decision tree is called the root node. It represents the entire dataset and is divided into two or more child nodes based on the value of a selected feature.

3. **Decision Nodes**:
   - Decision nodes are internal nodes of the tree where decisions are made based on the values of features. Each decision node represents a specific feature and a corresponding decision rule.

4. **Leaf Nodes**:
   - Leaf nodes are terminal nodes of the tree where the final outcome or prediction is made. Each leaf node represents a class label or a numerical value (depending on the type of problem) assigned to the observation that reaches that node.

5. **Splitting Criteria**:
   - Splitting criteria determine how the decision tree algorithm chooses the best feature to split the data at each decision node. Common splitting criteria include Gini impurity, entropy, and information gain, which aim to maximize the homogeneity (or purity) of the resulting subsets.

6. **Pruning**:
   - Pruning is a technique used to prevent overfitting in decision trees by removing nodes that do not significantly improve the performance of the tree on unseen data. It helps simplify the tree structure and improve its generalization ability.

7. **Tree Depth**:
   - The depth of a decision tree refers to the length of the longest path from the root node to a leaf node. Deeper trees may capture more complex patterns in the data but are also more prone to overfitting.

8. **Classification vs. Regression Trees**:
   - Decision trees can be used for both classification and regression tasks. In classification trees, the leaf nodes represent class labels, while in regression trees, the leaf nodes represent numerical values.

9. **Interpretability**:
   - One of the key advantages of decision trees is their interpretability. The decision rules learned by the tree can be easily understood and visualized, making them useful for explaining and communicating insights from the data.

10. **Ensemble Methods**:
    - Decision trees are often used as building blocks in ensemble learning methods such as Random Forest and Gradient Boosting, where multiple decision trees are combined to improve predictive performance.

Decision trees provide a flexible and intuitive approach to predictive modeling, suitable for a wide range of applications in fields such as machine learning, data mining, and pattern recognition.

